{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d03b9bd",
   "metadata": {},
   "source": [
    "# Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19892ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 설정값 ---\n",
    "TOTAL_STEPS = 20000\n",
    "ENV_ID = 'ALE/MontezumaRevenge-v5'\n",
    "\n",
    "print(f\"'{ENV_ID}' 환경에서 총 {TOTAL_STEPS} 스텝의 데이터를 수집합니다.\")\n",
    "\n",
    "# --- 환경 초기화 ---\n",
    "env = gym.make(ENV_ID, render_mode='rgb_array')\n",
    "\n",
    "# --- 데이터를 종류별로 저장할 리스트 생성 ---\n",
    "obs_buffer = []\n",
    "action_buffer = []\n",
    "reward_buffer = []\n",
    "terminated_buffer = []\n",
    "truncated_buffer = []\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in tqdm(range(TOTAL_STEPS)):\n",
    "    action = env.action_space.sample()\n",
    "    next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # 각 리스트에 데이터 추가\n",
    "    obs_buffer.append(observation)\n",
    "    action_buffer.append(action)\n",
    "    reward_buffer.append(reward)\n",
    "    terminated_buffer.append(terminated)\n",
    "    truncated_buffer.append(truncated)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "    else:\n",
    "        observation = next_observation\n",
    "\n",
    "env.close()\n",
    "print(f\"\\n데이터 수집 완료! 총 {len(obs_buffer)}개의 경험을 저장했습니다.\")\n",
    "\n",
    "# --- 압축된 .npz 파일로 저장 ---\n",
    "np.savez_compressed(\n",
    "    'data/montezuma_data1.npz',  # 파일 이름\n",
    "    observations=np.array(obs_buffer, dtype=np.uint8),\n",
    "    actions=np.array(action_buffer, dtype=np.int8),\n",
    "    rewards=np.array(reward_buffer, dtype=np.float32),\n",
    "    terminateds=np.array(terminated_buffer, dtype=np.bool_),\n",
    "    truncateds=np.array(truncated_buffer, dtype=np.bool_)\n",
    ")\n",
    "\n",
    "print(\"✅ 'montezuma_data1.npz' 파일로 데이터 저장을 완료했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d029a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('data/montezuma_data1.npz')\n",
    "observations_raw = data['observations']\n",
    "actions = data['actions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0985f6e3",
   "metadata": {},
   "source": [
    "# VAE train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af45596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/world_model/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "40it [00:04,  9.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Everage loss: 8505.401709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:03, 10.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Everage loss: 8392.880701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:03, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Everage loss: 6667.244287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:03, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Everage loss: 4835.549268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:03, 10.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Everage loss: 4151.997900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:03, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Everage loss: 3850.192230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:03, 10.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Everage loss: 3585.904987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:03, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Everage loss: 3304.241467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:03, 10.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Everage loss: 3100.706262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [00:03, 10.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Everage loss: 2939.249133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from parts.VAE_CNN import VAE, vae_loss_function, CustomImageDataset\n",
    "from parts.MDN_RNN import MDN_RNN, mdn_rnn_loss\n",
    "from parts.controller import controller\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "observations = torch.tensor(observations_raw).permute(0, 3, 1, 2)\n",
    "vae = VAE(input_channel=3, latent_dim=256).to('cuda:0')\n",
    "\n",
    "resize = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()]) \n",
    "\n",
    "observations = CustomImageDataset(data=observations, transform=resize)\n",
    "\n",
    "dataloader = DataLoader(dataset=observations, batch_size=512)\n",
    "\n",
    "optimizer = optim.AdamW(vae.parameters(), lr=1e-4)\n",
    "\n",
    "def vae_train(vae, optimizer, dataloader, epochs=10):\n",
    "    vae.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, data in tqdm(enumerate(dataloader)):\n",
    "            data = data.to('cuda:0')\n",
    "            _, recon_image, mu, logvar = vae(data)\n",
    "\n",
    "            loss = vae_loss_function(recon_image, data, mu, logvar, beta=1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch}, Everage loss: {total_loss/len(dataloader):.6f}')\n",
    "\n",
    "vae_train(vae, optimizer, dataloader, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e6442fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_pil = transforms.ToPILImage()\n",
    "to_pil(observations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc17b947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAAEADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDwOSR2kO52JyepzTMn1NXf7LuGmaMNGWGf4x+VLJpFzGSGMYIAJ+cUybxKkWWkA37R65qyyNEeJ+g7N/8AX/zipG0edEDGSHB9JAefSlOlSYJ82Pgf3x+VAm0+oWkwEy+ZOyqOD83Tp/n8KrXUzNJxIxAyPve/1+lWG0mUAkSxHA7OPypo0mdlYh4vlGSPMH5fWjUS5e5RyfU1NaOyXcJDEEOO/vVoaLdE4zDnGf8AWD8vrRDpk6XcavszuHG4HvQtxuUbMbEWN+Nzt97nn3ovnKzsqs2F4PPv7HpWzLoEtnfBpXTH3jk9OT1/Kql7Has7EuA+cHkYHWhrUm6uY3mvx8x46c0ea+PvH86sPDEB8rgnv0q5puj/AG8kLIoIGeT1/wA/0pFXRlb2/vH86USOOjEfjWlqOkGwdVLg5Geo469ajt7KKSIs0qqR2Jp2C6G6eS9ygLkAe5p8RYarGoJwJAMZ/wBqprRIbebd5gJHHbr+dX9M0n7ZfRyIwPzZxn3zihbktqxe8VzvHdELu4BGfTk/pXGOzE5Oa9h1PT7abLywq2FOSQO2e1cPrFtYwWh2wgSBsZ444PWhopWRyg610/hqLd5jlsYAGM9eQf6Z/CuayA3A/Cr9pqhtQQqDkYP+c/5xSCd7aGtr8RWQLvztBHB4Xk8fnWGFIQ/MQRwRmp7rVmueWjUHBHH4/wCNUll556d6YknbUT5ge9dZ4RR3vFyDjA/Hkf5/CmaLa2VzabpY8v07AHp/LNdxpOn21tEPKjAJGCfX0/KmhOzJbyRnifuef5GuM1mwnnt32xtuVufYYPH+farN54qltbl0NoWAyDuX0J9/1qqfGsj532EZDD5hs+uP59aNBJvsc4uh3rNt8lsgc8HirI8NXoRiYyCB6VvJ41fvp0eWHzYTr6fz607/AITaRvvWC/MCW+X8v/10WXcrmfY59vDd4q58s5A6Y6VWOi3kbENEwwOeK6o+NpD96wT5h83y/wCfzqJ/GbN97TozuBLZTr/n1osu4cz7BpOnzw2u0qwzjgAj0/w967WyPlxqjDG0Af5/L9K4o+N5Wx/oCDIO75T/AI+/WrOmeJbi/v4YzalS55wD0z/9ehWJ1XQ//9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# latent_size 256, beta=0.2\n",
    "vae.eval()\n",
    "data = observations[10044].unsqueeze(0).to(\"cuda:0\")\n",
    "hidden_state, recon_image, _, _ = vae(data)\n",
    "to_pil(recon_image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1381b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), 'model_weights/vae_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d602a484",
   "metadata": {},
   "source": [
    "# MDN_RNN train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd6cd3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from parts.VAE_CNN import VAE\n",
    "vae = VAE(input_channel=3, latent_dim=256).to('cuda:0')\n",
    "vae.load_state_dict(torch.load('model_weights/vae_weights.pth'))\n",
    "vae.eval()\n",
    "import numpy as np\n",
    "data = np.load('data/montezuma_data1.npz')\n",
    "observations_raw = data['observations']\n",
    "actions = data['actions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad1a449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "observations = torch.tensor(observations_raw).permute(0, 3, 1, 2)\n",
    "\n",
    "resize = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()]) \n",
    "\n",
    "import torch\n",
    "from parts.VAE_CNN import VAE\n",
    "vae = VAE(input_channel=3, latent_dim=256).to('cuda:0')\n",
    "vae.load_state_dict(torch.load('model_weights/vae_weights.pth'))\n",
    "vae.eval()\n",
    "import numpy as np\n",
    "data = np.load('data/montezuma_data1.npz')\n",
    "observations_raw = data['observations']\n",
    "actions = data['actions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00696145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab64a48664c4978b63f196008b44910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch: 1:   0%|          | 0/396 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 368.72986694335935\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     49\u001b[39m                 \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m(batch_idx\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mepoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Everage loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss/\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43mrnn_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mrnn_train\u001b[39m\u001b[34m(model, dataloader, optimizer, epochs)\u001b[39m\n\u001b[32m     30\u001b[39m     z_vectors_flatten, _, _, _ = vae(reshape_images)\n\u001b[32m     31\u001b[39m     z_vectors = z_vectors_flatten.view(batch, sequence, -\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m mu, sigma, phi = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m batch, sequence, num_dist, latent_size = mu.size()\n\u001b[32m     37\u001b[39m mu_pred = mu[: , :-\u001b[32m1\u001b[39m, :].reshape(-\u001b[32m1\u001b[39m, num_dist, latent_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/world_model/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/world_model/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/world_model/parts/MDN_RNN.py:47\u001b[39m, in \u001b[36mMDN_RNN.forward\u001b[39m\u001b[34m(self, input_z_vector, a_t_onehot)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_z_vector, a_t_onehot):\n\u001b[32m     46\u001b[39m     vector_sequence = torch.cat([input_z_vector, a_t_onehot], dim=\u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     output, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector_sequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m     B, Seq, hidden_size = output.size()\n\u001b[32m     49\u001b[39m     output = output.reshape(-\u001b[32m1\u001b[39m, hidden_size) \u001b[38;5;66;03m# predict at hidden states of all sequences\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/world_model/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/world_model/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/world_model/.venv/lib/python3.11/site-packages/torch/nn/modules/rnn.py:1124\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1121\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.permute_hidden(hx, sorted_indices)\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     result = _VF.lstm(\n\u001b[32m   1137\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1138\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1145\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1146\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from parts.MDN_RNN import MDN_RNN, mdn_rnn_loss, SequenceDataset\n",
    "import numpy as np\n",
    "\n",
    "action_size = int(actions.max()) + 1\n",
    "action_onehot = np.eye(action_size)[actions] # np.eye is generates identity matrix\n",
    "\n",
    "mdn_rnn = MDN_RNN(input_size=256, action_size=action_size).to('cuda:0')\n",
    "\n",
    "seq_dataset = SequenceDataset(image_dataset=observations, transforms=resize, action_dataset=action_onehot, sequence_length=1000)\n",
    "dataloader = DataLoader(dataset=seq_dataset, batch_size=48, num_workers=16)\n",
    "\n",
    "optimizer = optim.AdamW(mdn_rnn.parameters(), lr=1e-4)\n",
    "\n",
    "def rnn_train(model=mdn_rnn, dataloader=dataloader, optimizer=optimizer, epochs=10):\n",
    "    model.train()\n",
    "    vae.eval()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (image, action) in enumerate(tqdm(dataloader, desc=f'epoch: {epoch+1}')):\n",
    "            image, action = image.to('cuda:0'), action.to('cuda:0')\n",
    "            with torch.no_grad():\n",
    "                # have to reshape image vector because vae(cnn) input shape is (batch_size, channel_size, height, width)\n",
    "                batch, sequence, C, H, W = image.size()\n",
    "                reshape_images = image.view(-1, C, H, W)\n",
    "                z_vectors_flatten, _, _, _ = vae(reshape_images)\n",
    "                z_vectors = z_vectors_flatten.view(batch, sequence, -1)\n",
    "                \n",
    "            mu, sigma, phi = model(z_vectors, action)\n",
    "\n",
    "            batch, sequence, num_dist, latent_size = mu.size()\n",
    "\n",
    "            mu_pred = mu[: , :-1, :].reshape(-1, num_dist, latent_size)\n",
    "            sigma_pred = sigma[: , :-1, :].reshape(-1, num_dist, latent_size)\n",
    "            phi_pred = phi[: , :-1, :].reshape(-1, num_dist)\n",
    "            target_z =  z_vectors[: , 1:, :].reshape(-1, latent_size)\n",
    "\n",
    "            loss = mdn_rnn_loss(mu_pred, sigma_pred, phi_pred, target_z)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f\"loss {total_loss / (batch_idx + 1)}\")\n",
    "\n",
    "        print(f\"epoch: {epoch+1}, Everage loss: {total_loss/len(dataloader):.6f}\")\n",
    "\n",
    "rnn_train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "world-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
