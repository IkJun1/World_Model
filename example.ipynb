{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19892ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 설정값 ---\n",
    "TOTAL_STEPS = 20000\n",
    "ENV_ID = 'ALE/MontezumaRevenge-v5'\n",
    "\n",
    "print(f\"'{ENV_ID}' 환경에서 총 {TOTAL_STEPS} 스텝의 데이터를 수집합니다.\")\n",
    "\n",
    "# --- 환경 초기화 ---\n",
    "env = gym.make(ENV_ID, render_mode='rgb_array')\n",
    "\n",
    "# --- 데이터를 종류별로 저장할 리스트 생성 ---\n",
    "obs_buffer = []\n",
    "action_buffer = []\n",
    "reward_buffer = []\n",
    "terminated_buffer = []\n",
    "truncated_buffer = []\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in tqdm(range(TOTAL_STEPS)):\n",
    "    action = env.action_space.sample()\n",
    "    next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # 각 리스트에 데이터 추가\n",
    "    obs_buffer.append(observation)\n",
    "    action_buffer.append(action)\n",
    "    reward_buffer.append(reward)\n",
    "    terminated_buffer.append(terminated)\n",
    "    truncated_buffer.append(truncated)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "    else:\n",
    "        observation = next_observation\n",
    "\n",
    "env.close()\n",
    "print(f\"\\n데이터 수집 완료! 총 {len(obs_buffer)}개의 경험을 저장했습니다.\")\n",
    "\n",
    "# --- 압축된 .npz 파일로 저장 ---\n",
    "np.savez_compressed(\n",
    "    'data/montezuma_data1.npz',  # 파일 이름\n",
    "    observations=np.array(obs_buffer, dtype=np.uint8),\n",
    "    actions=np.array(action_buffer, dtype=np.int8),\n",
    "    rewards=np.array(reward_buffer, dtype=np.float32),\n",
    "    terminateds=np.array(terminated_buffer, dtype=np.bool_),\n",
    "    truncateds=np.array(truncated_buffer, dtype=np.bool_)\n",
    ")\n",
    "\n",
    "print(\"✅ 'montezuma_data1.npz' 파일로 데이터 저장을 완료했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('data/montezuma_data1.npz')\n",
    "observations_raw = data['observations']\n",
    "actions = data['actions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af45596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from parts.VAE_CNN import VAE, vae_loss_function, CustomImageDataset\n",
    "from parts.MDN_RNN import MDN_RNN, mdn_rnn_loss\n",
    "from parts.controller import controller\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "observations = torch.tensor(observations_raw).permute(0, 3, 1, 2)\n",
    "vae = VAE(input_channel=3, latent_dim=256).to('cuda:0')\n",
    "\n",
    "resize = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()]) \n",
    "\n",
    "observations = CustomImageDataset(data=observations, transform=resize)\n",
    "\n",
    "dataloader = DataLoader(dataset=observations, batch_size=512)\n",
    "\n",
    "optimizer = optim.AdamW(vae.parameters(), lr=1e-4)\n",
    "\n",
    "def vae_train(vae, optimizer, dataloader, epochs=10):\n",
    "    vae.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, data in tqdm(enumerate(dataloader)):\n",
    "            data = data.to('cuda:0')\n",
    "            _, recon_image, mu, logvar = vae(data)\n",
    "\n",
    "            loss = vae_loss_function(recon_image, data, mu, logvar, beta=0.2)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch}, Everage loss: {total_loss/len(dataloader):.6f}')\n",
    "\n",
    "vae_train(vae, optimizer, dataloader, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6442fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil = transforms.ToPILImage()\n",
    "to_pil(observations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc17b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_size 256, beta=0.2\n",
    "vae.eval()\n",
    "data = observations[10044].unsqueeze(0).to(\"cuda:0\")\n",
    "hidden_state, recon_image, _, _ = vae(data)\n",
    "to_pil(recon_image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1a449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = torch.tensor(observations_raw).permute(0, 3, 1, 2)\n",
    "\n",
    "resize = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00696145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from parts.MDN_RNN import MDN_RNN, mdn_rnn_loss, SequenceDataset\n",
    "import numpy as np\n",
    "\n",
    "action_size = int(actions.max()) + 1\n",
    "action_onehot = np.eye(action_size)[actions] # np.eye is generates identity matrix\n",
    "\n",
    "mdn_rnn = MDN_RNN(input_size=256, action_size=action_size).to('cuda:0')\n",
    "\n",
    "seq_dataset = SequenceDataset(image_dataset=observations, transforms=resize, action_dataset=action_onehot, sequence_length=1000)\n",
    "dataloader = DataLoader(dataset=seq_dataset, batch_size=8)\n",
    "\n",
    "optimizer = optim.AdamW(mdn_rnn.parameters(), lr=1e-4)\n",
    "\n",
    "def rnn_train(model=mdn_rnn, dataloader=dataloader, optimizer=optimizer, epochs=10):\n",
    "    model.train()\n",
    "    vae.eval()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (image, action) in enumerate(dataloader):\n",
    "            image, action = image.to('cuda:0'), action.to('cuda:0')\n",
    "            with torch.no_grad():\n",
    "                # have to reshape image vector because vae(cnn) input shape is (batch_size, channel_size, height, width)\n",
    "                batch, sequence, C, H, W = image.size()\n",
    "                reshape_images = image.view(-1, C, H, W)\n",
    "                z_vectors_flatten, _, _, _ = vae(reshape_images)\n",
    "                z_vectors = z_vectors_flatten.view(batch, sequence, -1)\n",
    "                \n",
    "            mu, sigma, phi = model(z_vectors, action)\n",
    "\n",
    "            batch, sequence, num_dist, latent_size = mu.size()\n",
    "\n",
    "            mu_pred = mu[: , :-1, :].view(-1, num_dist, latent_size)\n",
    "            sigma_pred = sigma[: , :-1, :].view(-1, num_dist, latent_size)\n",
    "            phi_pred = phi[: , :-1, :].view(-1, num_dist)\n",
    "            target_z =  z_vectors[: , 1:, :].view(-1, latent_size)\n",
    "\n",
    "            loss = mdn_rnn_loss(mu_pred, sigma_pred, phi_pred, target_z)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"epoch: {epoch+1}, Everage loss: {total_loss/len(dataloader):.6f}\")\n",
    "\n",
    "rnn_train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
