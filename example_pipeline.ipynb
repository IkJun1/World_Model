{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d03b9bd",
   "metadata": {},
   "source": [
    "# Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19892ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# --- 설정값 ---\n",
    "TOTAL_STEPS = 20000\n",
    "ENV_ID = 'LunarLander-v3'\n",
    "\n",
    "print(f\"'{ENV_ID}' 환경에서 총 {TOTAL_STEPS} 스텝의 데이터를 수집합니다.\")\n",
    "\n",
    "# --- 환경 초기화 ---\n",
    "env = gym.make(ENV_ID, render_mode='rgb_array')\n",
    "\n",
    "# --- 데이터 버퍼 ---\n",
    "obs_buffer = []\n",
    "frame_buffer = []\n",
    "action_buffer = []\n",
    "reward_buffer = []\n",
    "terminated_buffer = []\n",
    "truncated_buffer = []\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in tqdm(range(TOTAL_STEPS)):\n",
    "    action = env.action_space.sample()\n",
    "    next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # 버퍼에 저장\n",
    "    obs_buffer.append(observation)  # 상태 (벡터)\n",
    "    frame = env.render()            # 프레임 이미지\n",
    "    frame_buffer.append(frame)     # 이미지 추가\n",
    "\n",
    "    action_buffer.append(action)\n",
    "    reward_buffer.append(reward)\n",
    "    terminated_buffer.append(terminated)\n",
    "    truncated_buffer.append(truncated)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "    else:\n",
    "        observation = next_observation\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(f\"\\n데이터 수집 완료! 총 {len(obs_buffer)}개의 경험을 저장했습니다.\")\n",
    "\n",
    "# --- 저장 디렉토리 만들기 ---\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# --- 저장 ---\n",
    "np.savez_compressed(\n",
    "    f\"data/{ENV_ID}_with_frames_1.npz\",\n",
    "    observations=np.array(obs_buffer, dtype=np.float32),  # (100000, 8)\n",
    "    frames=np.array(frame_buffer, dtype=np.uint8),        # (100000, H, W, 3)\n",
    "    actions=np.array(action_buffer, dtype=np.int8),\n",
    "    rewards=np.array(reward_buffer, dtype=np.float32),\n",
    "    terminateds=np.array(terminated_buffer, dtype=np.bool_),\n",
    "    truncateds=np.array(truncated_buffer, dtype=np.bool_)\n",
    ")\n",
    "\n",
    "print(f\"✅ '{ENV_ID}_with_frames_1.npz' 파일로 프레임 포함 데이터 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d029a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('data/LunarLander-v3.npz')\n",
    "observations_raw = data['frames']\n",
    "actions = data['actions']\n",
    "rewards = data['rewards']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0985f6e3",
   "metadata": {},
   "source": [
    "# VAE train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af45596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from parts.VAE_CNN import VAE, vae_loss_function, CustomImageDataset\n",
    "from parts.MDN_RNN import MDN_RNN, mdn_rnn_loss\n",
    "from parts.controller import controller\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "observations = torch.tensor(observations_raw).permute(0, 3, 1, 2)\n",
    "vae = VAE(input_channel=3, latent_dim=1024).to('cuda:0')\n",
    "\n",
    "resize = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()]) \n",
    "\n",
    "observations = CustomImageDataset(data=observations, transform=resize)\n",
    "\n",
    "dataloader = DataLoader(dataset=observations, batch_size=512)\n",
    "\n",
    "optimizer = optim.AdamW(vae.parameters(), lr=1e-4)\n",
    "\n",
    "def vae_train(vae, optimizer, dataloader, epochs=10):\n",
    "    vae.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, data in enumerate(tqdm(dataloader)):\n",
    "            data = data.to('cuda:0')\n",
    "            _, recon_image, mu, logvar = vae(data)\n",
    "\n",
    "            loss = vae_loss_function(recon_image, data, mu, logvar, beta=0.01)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch}, Everage loss: {total_loss/len(dataloader):.6f}')\n",
    "\n",
    "vae_train(vae, optimizer, dataloader, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9365bc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), 'vae-latent1024-epoch100.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e6442fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAAEADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD5/ooooAKKKKALa6ZePp736QM1qjbWlHQH/JqoRg4rai8RXCaQ+ltFCbNxgqFwwPrn6jNYtaTUFbkfr6gFFFFZgFFFFABRRRQAUUUUAFFFFABRTnRo3ZHUqynBVhgg02gApSCMZBGaSrl1dXdzDbwyySPFBGFiU9FHXik730AqAEnABJ9qSrVjNcWV2txAzxSxhirgcg4NV5JHlkaSRizscsT1Jo1v5ANooopgfY/jL4UeGvGkguLqBrS9yM3VrhWcejDGD9TzWRZ/APwPbR7Zre8um/vS3JH/AKDivT6KAOK0v4SeB9JnE0GgwSSDobhmlA/BiRXWrYWaIES0gVQMACMAAflViigCu1hZuhV7SBlIwQYwQR+VcB4t+C/hXxHaytZ2cel355Se2XC5/wBpOhH0wa9HooA+IfFvgrWvBepGz1a2KqSfKuE5jlHqp/p1rn0RpHCIpZmOAoGSTX3frOiab4g02XT9VtI7q1kHKOOh9Qex9xXK+EPhP4Y8HXcl5aW73N2WJjnuiHaIdgvGB9etAH//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAAD6UlEQVR4Ae1azU8aQRTfXRZ2+doKKChtTdSWojWYHnrAiBebJqY3Exu9ePHgybuX/gN6kMSLejCpFy8mhpMxUaMnE6lJDfEjNiamNQ3UFFFwpVigD0lXXNllwaVd4uyBzMebN7/fb968meyCYehBCihDAZIgtGq1MrCUhILAcTVB3B1K4jh+t7WCWhqMRiBG4CpKYyAIkkMuFyu5/HDA+AVYgHQ6XWN6/tb94SL+8/vJ58AX32U88livD11e/k6l+AOKrN9IUuRAqeaAHkyt5hcNdg9JUI6nb74FPwGBIMsmr7ukOhKwyxO1ApalN9MUY7e6VIQGXEAs4XhGNVnQZxyWjkvaSBwnXj57ZzU7/wYrrqWZ3M0gzY2gVdlDSK+10NQj/87HSPRrjcmpIbX1da9/hA+isZAgKEV1QPKxVDUAJAgeiCUTU/+q+b2tullRIIsDY7U4qpgnxY1RgnU1TetI+SO27JuY0y6RSqXkyJucQ1R4IAroSVKT75InF/2y7wE4ce973ZGLK/KDFEAKIAWQAkiBB6tA+V4Olf0qkV0zLUWVj8ODDQtEHCmAFEAKIAWQArcUMJkwmw0r+PYNbMxmTKW6NbbkipxXiY4ObGICGx7G6uoE8cAHs/5+zOvFBgYwhhE0k94h58vKkxPt/Pyv7e1UOCwG4PBQd3DA7uxgLCtm9h/6bDYbWSiA4Epnt9tlBCdnCMkIS7orREC6VuWxrPgVIHt7e5eWls7Pz+8pEOxOiqIYhkkmkyKuwIymafjNfj8WsZTYhcdiscXFxbGxsa2tLfG583pUqVQWi6WlpaWzs9Pj8RiNRnFkAJ1l2fX19ZWVld3d3dPTU3H7vJPmNmaUgOf4+HhycnJmZiYYDOZ2QxmmhOSoVmf+8cA9gBvEbmtr6+rqcrvdTU1NAB0seWOFqjDj2dnZ3t7e6urq8vJyIBAIh8PQKGQv0n6zlFdXVxsbG6Ojo2tra1ldIWE3NjY6nU6Hw2EymYADMMk+UAYCtbW1EDYi3gt2AehoNApL4fV6fT5fPB4vOIRncEMg2wFKbG5ugpyAHmJDp4Mvi3Ke1rzpuWokEpmamhofHw+FivwADhoo5EkkEpBO2tvbYf05YoULCkHPwTg6OhoaGtLr9YWhZy24kcopQGKcnp6GxCApKygHdy4SSOh+v7+7uxuyhdBSGAyGnp4eLHeY0sqwoUdGRiAB8jhANm9tbZ2dnc2cv0oDzcMDiXVhYQEOHC6cIEMODg7u7+9f/3UhrXQCwAeAwpHX19cHOR2YzM3NXVxccDz55wBvsZRThYMCjm2Xy8Xb3BVDQEjKir9OIwJCS/uv2v8AxS5KQ9w20ecAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_pil = transforms.ToPILImage()\n",
    "to_pil(observations[555])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc17b947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAAEADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD5/ooooAKKKKAL6aNfPpT6ksDG1U4L4qhW1b+Jr+30OXSQwNtJ1BHSsWtKihpyfMAooorMAooooAKKKKACiiigAooooAKKdJG8TlJFKsDggjBFNoAKUgjqOtJVy7u7y6ht4JpZJIoIwsSnog68Une6sBUAJOACT7UlW9PnubG8W5t2eOaNWKuByDtNVpJHmlaSRizuSzMepJo1v5ANooopgfaOu/DPwn4i+a90mIS/89Yfkb8x1/GsmL4I+A41wdJeT3e4f+hr0SigDitL+E3gnSLgzwaFBI/b7QTKB9AxIrrksrSNQqW0KqBgARgAVPRQBAbK1YENbQkEYIKDmuP8S/Cnwn4ksZYW0uCyuG5S5tIxG6N64HB+hrt6KAPi3xv8PNb8DX5jvojLZs2IbyNT5b+3sfY1ysUUk8qxxIzuxwFUZJNfeOo6bZatYS2OoW0VzayjDxSqGBrmPC3wx8M+Eria5sbISXEjHEs/zsi/3Vz0oA//2Q==",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.eval()\n",
    "data = observations[555].unsqueeze(0).to(\"cuda:0\")\n",
    "hidden_state, recon_image, _, _ = vae(data)\n",
    "to_pil(recon_image[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d602a484",
   "metadata": {},
   "source": [
    "# MDN_RNN train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd6cd3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('data/LunarLander-v3.npz')\n",
    "observations_raw = data['frames']\n",
    "actions = data['actions']\n",
    "rewards = data['rewards']\n",
    "\n",
    "episodes = [0]\n",
    "for idx, done in enumerate(data['terminateds']):\n",
    "    if done == True:\n",
    "        episodes.append(idx+1)\n",
    "episodes.append(len(data['terminateds']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad1a449e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encode_cnn): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): SiLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): SiLU()\n",
       "    (4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (5): SiLU()\n",
       "    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (7): SiLU()\n",
       "  )\n",
       "  (mu_linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (var_linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (decode_linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (decode_cnn): Sequential(\n",
       "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): SiLU()\n",
       "    (2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): SiLU()\n",
       "    (4): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (5): SiLU()\n",
       "    (6): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "# observations = torch.tensor(observations_raw).permute(0, 3, 1, 2)\n",
    "observations = observations_raw # numpy array dimension order is [H, W, C] for ToPILImage()\n",
    "\n",
    "resize = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()]) \n",
    "\n",
    "import torch\n",
    "from parts.VAE_CNN import VAE\n",
    "vae = VAE(input_channel=3, latent_dim=1024).to('cuda:0')\n",
    "vae.load_state_dict(torch.load('model_weights/vae_weights_latent1024.pth'))\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4895f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from parts.MDN_RNN import MDN_RNN, mdn_rnn_loss, SequenceDataset\n",
    "import numpy as np\n",
    "\n",
    "action_size = int(actions.max()) + 1\n",
    "action_onehot = np.eye(action_size)[actions] # np.eye is generates identity matrix\n",
    "\n",
    "mdn_rnn = MDN_RNN(input_size=1024, hidden_size=512, latent_space_size=1024, action_size=action_size).to('cuda:0')\n",
    "\n",
    "seq_dataset = SequenceDataset(image_dataset=observations, transforms=resize, reward_dataset=rewards, action_dataset=action_onehot, episodes=episodes)\n",
    "dataloader = DataLoader(dataset=seq_dataset, batch_size=32, num_workers=12)\n",
    "\n",
    "optimizer = optim.AdamW(mdn_rnn.parameters(), lr=1e-4)\n",
    "\n",
    "def rnn_train(model=mdn_rnn, dataloader=dataloader, optimizer=optimizer, epochs=400):\n",
    "    model.train()\n",
    "    vae.eval()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (image, action, reward, seq_length, mask, t_done) in enumerate(tqdm(dataloader, desc=f'epoch: {epoch+1}')):\n",
    "            image, action, reward, mask, t_done = image.to('cuda:0'), action.to('cuda:0'), reward.to('cuda:0'), mask.to('cuda:0'), t_done.to('cuda:0')\n",
    "            with torch.no_grad():\n",
    "                # have to reshape image vector because vae(cnn) input shape is (batch_size, channel_size, height, width)\n",
    "                batch, sequence, C, H, W = image.size()\n",
    "                reshape_images = image.view(-1, C, H, W)\n",
    "                z_vectors_flatten, _, _, _ = vae(reshape_images)\n",
    "                z_vectors = z_vectors_flatten.view(batch, sequence, -1)\n",
    "                \n",
    "            mu, sigma, phi, p_reward, p_done, _, (_, _) = model(z_vectors, action, length=seq_length)\n",
    "            \n",
    "            batch, sequence, num_dist, latent_size = mu.size()\n",
    "\n",
    "            mu_pred = mu[: , :-1, :, :].reshape(-1, num_dist, latent_size)\n",
    "            sigma_pred = sigma[: , :-1, :, :].reshape(-1, num_dist, latent_size)\n",
    "            phi_pred = phi[: , :-1, :].reshape(-1, num_dist)\n",
    "            p_reward_pred = p_reward[:, :-1].reshape(-1, 1)\n",
    "\n",
    "            target_z = z_vectors[: , 1:, :].reshape(-1, latent_size)\n",
    "            target_reward = reward[:, 1:].reshape(-1, 1)\n",
    "\n",
    "            reshape_mask = mask[:, :].reshape(-1, 1)\n",
    "            \n",
    "            loss = mdn_rnn_loss(mu_pred, sigma_pred, phi_pred, target_z, p_reward_pred, target_reward, p_done, t_done, reshape_mask)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f\"loss {total_loss / (batch_idx + 1)}\")\n",
    "\n",
    "        print(f\"epoch: {epoch+1}, Everage loss: {total_loss/len(dataloader):.6f}\")\n",
    "\n",
    "rnn_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d0288",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mdn_rnn.state_dict(), 'model_weights/mdnrnn_weights_latent1024.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f70decd",
   "metadata": {},
   "source": [
    "# RNN test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bc01a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('data/LunarLander-v3.npz')\n",
    "observations_raw = data['frames']\n",
    "actions = data['actions']\n",
    "rewards = data['rewards']\n",
    "\n",
    "episodes = [0]\n",
    "for idx, done in enumerate(data['terminateds']):\n",
    "    if done == True:\n",
    "        episodes.append(idx+1)\n",
    "episodes.append(len(data['terminateds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6bc3e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from parts.VAE_CNN import VAE, vae_loss_function, CustomImageDataset\n",
    "from parts.MDN_RNN import MDN_RNN, mdn_rnn_loss, sampling, SequenceDataset\n",
    "from parts.controller import controller\n",
    "\n",
    "action_size = int(actions.max()) + 1\n",
    "action_onehot = np.eye(action_size)[actions] # np.eye is generates identity matrix\n",
    "\n",
    "vae = VAE(input_channel=3, latent_dim=1024).to('cuda:0')\n",
    "mdn_rnn = MDN_RNN(input_size=1024, hidden_size=512, latent_space_size=1024, action_size=action_size).to('cuda:0')\n",
    "\n",
    "vae.load_state_dict(torch.load('model_weights/vae_weights_latent1024.pth'))\n",
    "mdn_rnn.load_state_dict(torch.load('model_weights/mdnrnn_weights_latent1024.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9db214c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "observations = observations_raw\n",
    "\n",
    "resize = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()]) \n",
    "\n",
    "\n",
    "seq_dataset = SequenceDataset(image_dataset=observations, transforms=resize, reward_dataset=rewards, action_dataset=action_onehot, episodes=episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec62d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "mdn_rnn.eval()\n",
    "z_vectors, _, _, _  = vae(seq_dataset[100][0].to('cuda:0'))\n",
    "z_vectors = z_vectors.unsqueeze(0)\n",
    "action_first = seq_dataset[100][1].to('cuda:0')\n",
    "action_first = action_first.unsqueeze(0)\n",
    "\n",
    "mu, sigma, phi, reward, h, cell = mdn_rnn(z_vectors, action_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0e1558f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAAEADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD5/ooooAKKKKACilPtSUAFFFFABRRRQAV0drfaXaeE2gns1lu55SQenAPXPbuMVzlGTjGeBWlOo4NtdrAB68dKKM5orMAooooAKKKciNI6oilnY4CqMkmgBtKQRjIIzXYzfCvxrBoseqtoVwYHGdi4Mqj1KfeH5Vzd7PeypDbXDSlLdAqRsMbPUY+tJ3voBSAJOACT7UlWrGa4srtbmBniljDFXA5BwarySPLI0jsWdjkk9SaNb+QDaKnt7O6u2221tNMfSNC38q3rH4e+L9Sx9l8OaiwP8TQFB+bYFMD1fwH+z+Zo1v8AxgzIGGUsInw3/A2HT6D869c0P4eeE/DlwlxpmiW0Vwn3ZmBdx9CxOK6eigAqFrS2dizW8RJ6koKmooArPp9lIjI9nbsjDDK0YII/KuJvfgx4HvdWTUG0nyyDloIZCkTn3Uf0xXf0UAVdP0yx0q1S10+zgtYEGFjhQKB+VWqKKAP/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAADs0lEQVR4Ae1ZzUsbQRSf3c3mQ5MmmoQkakkCgdoGGmOlTRviobYnCcVLIJCclNKLBy899Y8Q8VSwPYoQPHjQoyDY0lYkUs1BVEKwpY3UxDSf5qvPBkrWLjGJs90GZg+yMzvv937v994bZiJC5CEKEAWIAkQBogBRoKECFEIMBX8EeWhBULmgMobRyeXcuU4bCaW/0DooWVYrsPYSQWMolMulSkVQFwScTwGJREbTDN8X/HP4d6HuLt3j+y/Npgf4yfIhYu6BG0rjU9erOxavTm07y3zJZH8UztN8frHN4cwATTEP7z63W58xNGs1uR85XtyzB1hWgY0sHxDOACrVSjafoNDFpk9RzM9MPJdPKrv0fH6xzeEtoWrs28d8MSVhZJ8ibz98fnNezFSrwm6jeANAxVL2e2Iv+vX9u/Dr86Kw1Y8tifVAztu+W9YnrETYuq/3iLMHADdXSOUKZ8VSrt5HJ73fUJpU3YZ6xhKKgtNo/UyHvUtpWinB3GniSKCAA0ZHRwKXMlqwe5k4OSFeiQJEAaIAUYAoQBQgCtQpoNEggwFdeWCDNb29CNcRG+eFxuNBc3NoehqZTHVhcV/hOOf3o9lZFAwilYr7ra0RzpP6yYkiFCrs7FROTxtxOTzs2t/P7u2h3P92bzMYDJKrCoiiqL6+vkbxtfgNZwm16BrP8o4PoJ0egDoZHh52OBwbGxtHR0fFYhGPmC2iQDUyDNNaAGAzMDAwNTU1OTkJFX98fLy6urq0tLS9vZ1Op1sk0MLyGleZTKZSqXQ6nclkslgsNpvNbDa38M8rMPZ6vTMzM0NDQ3+atVqtJpPJzc3NxcXFSCQSi8XK5XIDakClp6cnGo2CYYNlcrm8v79fr9eDTPBy8/cDvI1Go1arBSawgKYv6r+pAKRSqdPpBOrj4+NKpZLXcT6fB/aJRKIxMwgglUotLy+vra1BAv+OFsi5XK5AIDA6OqrRaBQKBcuyNa68fnkCgNVgBsa1TA0ODtrtdo/HA9sfuOdFaXUS2ubg4GBlZSUUCu3u7kLwgAy1MTY2FgwG3W63Wq1uEpNDCFCgsKDER0ZGoNYBEfSAasHF+xInyFU8Hl9fX4eEgFh+vx82BtDu0rJmh2Dp8/nC4TCkFaD/5QMZgJy05/EiPBDYarXOz89DO7aHIqIVAuEnJia2trZKpZKIPNp2jRYWFmpbR9sQ4hpSlUpFoB5ttvmutw5+b+VsRNdDE8G64w9zJAARqobjkmSAI4cIA5IBEUTnuPwFpKq9AmyVJT4AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_pil = transforms.ToPILImage()\n",
    "to_pil(seq_dataset[100][0][88])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd559760",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = phi[:, 120, :]\n",
    "m = mu[:, 120, :, :]\n",
    "s = sigma[:, 120, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9de9795f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAAEADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD5/ooooAUHFSx3LxHKHBqGimnYCWW4eY5Y5NRUUUr3AKKKKACiiigAooooAKKKKACiiigAooooAKcUZQCVIB5GR1pyQSuu5Y2I9QKu3tze3MNvbyPK8UEYEaEcLxSd7qwGeqs5woJPoBQRir2my3Vjercwb45Y1Yq4HQ7TVSaSWeZ5pmZ5HO5mbqTRrfyAjooqWC3kuJVjjQsScYApgezX/wAAdUeTfZTR7WI4Y11Hh39nvTLQxy63dtdMOWij+Vfz617WBgUUAYWm+DfD+kxLFZ6XbIq9MoD/ADrVGn2Y6WkA/wC2Y/wqzRQBVfTbGRGR7O3ZWGCDEpBH5VwmtfBnwrq90ZltjahuWSA7Rn29K9FooA8hH7P/AIcEmPOuPLHfd8xrodC+E/hvQkbybbzZD/HIcmu8ooA//9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import distributions\n",
    "from torchvision import transforms\n",
    "\n",
    "mixture_distribution = distributions.Categorical(probs=p)\n",
    "component_distribution = distributions.Normal(loc=m, scale=s)\n",
    "component_dist = distributions.Independent(\n",
    "    component_distribution,\n",
    "    reinterpreted_batch_ndims=1\n",
    ")\n",
    "mixture_gaussian = distributions.MixtureSameFamily(mixture_distribution, component_dist)\n",
    "vector = mixture_gaussian.sample()\n",
    "\n",
    "to_pil = transforms.ToPILImage()\n",
    "a = vae.decode(vector)\n",
    "img = to_pil(a[0])\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c0e77",
   "metadata": {},
   "source": [
    "# Controller train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "676540e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encode_cnn): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): SiLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): SiLU()\n",
       "    (4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (5): SiLU()\n",
       "    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (7): SiLU()\n",
       "  )\n",
       "  (mu_linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (var_linear): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (decode_linear): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (decode_cnn): Sequential(\n",
       "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): SiLU()\n",
       "    (2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): SiLU()\n",
       "    (4): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (5): SiLU()\n",
       "    (6): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from parts.MDN_RNN import MDN_RNN, sampling\n",
    "from parts.controller import controller, choice_control\n",
    "from parts.VAE_CNN import VAE\n",
    "import torch\n",
    "\n",
    "data = np.load('data/LunarLander-v3.npz')\n",
    "\n",
    "actions = data['actions']\n",
    "action_size = int(actions.max()) + 1\n",
    "\n",
    "images = data['frames']\n",
    "\n",
    "mdn_rnn = MDN_RNN(input_size=1024, hidden_size=512, latent_space_size=1024, action_size=action_size).to('cuda:0')\n",
    "mdn_rnn.load_state_dict(torch.load('model_weights/mdnrnn_weights_latent1024.pth'))\n",
    "\n",
    "vae = VAE(input_channel=3, latent_dim=1024).to('cuda:0')\n",
    "vae.load_state_dict(torch.load('model_weights/vae_weights_latent1024.pth'))\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "257e0028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import transforms\n",
    "import random\n",
    "\n",
    "resize = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor()])\n",
    "\n",
    "def sellect_random_scene(images) :\n",
    "    _len = len(images)\n",
    "    scene_number = random.randint(0, _len-1)\n",
    "    image = images[scene_number]\n",
    "    \n",
    "    image_tensor = torch.tensor(image).permute(2, 0, 1)\n",
    "\n",
    "    resized_image = torch.tensor(resize(image_tensor)).unsqueeze(0).to('cuda:0')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z_vector, _ ,_ ,_ = vae(resized_image)\n",
    "\n",
    "    num_layers, hidden_size = mdn_rnn.lstm.num_layers, mdn_rnn.lstm.hidden_size\n",
    "\n",
    "    h_vector = torch.zeros(1, hidden_size).to('cuda:0')\n",
    "\n",
    "    h_n, c_n = torch.zeros(num_layers, 1, hidden_size).to('cuda:0'), torch.zeros(num_layers, 1, hidden_size).to('cuda:0')\n",
    "\n",
    "    return z_vector, h_vector, (h_n, c_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8658277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parts.MDN_RNN import MDN_RNN, sampling\n",
    "from parts.controller import controller, choice_control\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "C = controller(action_size=action_size, h_vector_size=512, z_vector_size=1024, hidden_size=512).to('cuda:0')\n",
    "\n",
    "optimizer = torch.optim.AdamW(C.parameters(), lr=1e-4)\n",
    "\n",
    "gamma = 0.99\n",
    "beta = 0.01 \n",
    "\n",
    "def controller_train(steps, controller, optimizer, initial_z, initial_h, cell):\n",
    "    controller.train()\n",
    "    mdn_rnn.train()\n",
    "\n",
    "    rewards = []\n",
    "    action_probs = []\n",
    "    entropies = []\n",
    "\n",
    "    z = initial_z.to('cuda:0')\n",
    "    h = initial_h.to('cuda:0')\n",
    "\n",
    "    z_rnn = z.unsqueeze(0)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        action_prob = controller(z, h)\n",
    "        action = choice_control(action_prob)\n",
    "\n",
    "        action_onehot_tensor = F.one_hot(action, num_classes=action_size).to(torch.float32)\n",
    "\n",
    "        mu, sigma, phi, reward, h, cell = mdn_rnn(z_rnn, action_onehot_tensor, cell)\n",
    "\n",
    "        z_rnn = sampling(mu, sigma, phi) \n",
    "        z = z_rnn.squeeze(0)\n",
    "\n",
    "        log_action_prob = torch.log(action_prob[0, action])\n",
    "\n",
    "        entropy = action_prob[0] * torch.log(action_prob[0])\n",
    "\n",
    "        # print(z)\n",
    "        # to_pil = transforms.ToPILImage()\n",
    "        # a = vae.decode(z)\n",
    "        # img = to_pil(a[0])\n",
    "        # plt.figure(figsize=(5, 5))\n",
    "        # plt.imshow(img)\n",
    "        # plt.axis('off')\n",
    "        # plt.show()\n",
    "        \n",
    "        \n",
    "        entropies.append(-entropy.sum())\n",
    "        action_probs.append(log_action_prob)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "    # total_imagined_reward = torch.stack(rewards)\n",
    "    \n",
    "    # total_action_prob = torch.stack(action_probs)\n",
    "    # total_entoprys = torch.stack(entropys).sum()\n",
    "\n",
    "    # loss_element_wise = total_action_prob * total_imagined_reward.detach() # reinforce algorithm\n",
    "    # loss = -loss_element_wise.sum() - total_entoprys*0.1\n",
    "\n",
    "    # optimizer.zero_grad()\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "\n",
    "    # return loss.item(), total_imagined_reward.sum(), total_action_prob.sum()\n",
    "    returns = []\n",
    "    future_return = 0.0\n",
    "    for r in reversed(rewards):\n",
    "        future_return = r + gamma * future_return\n",
    "        returns.append(future_return)\n",
    "    returns.reverse()\n",
    "    returns = torch.stack(returns)\n",
    "    \n",
    "    # 보상 정규화 (학습 안정화)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "    action_probs = torch.stack(action_probs)\n",
    "    entropies = torch.stack(entropies)\n",
    "\n",
    "    # <--- 엔트로피 보너스 구현 Step 2: 최종 손실 함수에 반영\n",
    "    policy_loss = - (action_probs * returns.detach()).mean() # 정책 손실\n",
    "    entropy_loss = - (beta * entropies.mean()) # 엔트로피 손실 (엔트로피를 최대화하기 위해 음수로 만듦)\n",
    "\n",
    "    loss = policy_loss + entropy_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # loss.item()과 함께 실제 보상 합계, 정책 손실, 엔트로피 손실 등을 리턴하면 모니터링에 용이\n",
    "    return loss.item(), torch.stack(rewards).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5087c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for _ in tqdm(range(1000)):\n",
    "    initial_z, initial_h, (h_n, c_n) = sellect_random_scene(images)\n",
    "    loss, reward = controller_train(50, C, optimizer, initial_z, initial_h, (h_n, c_n))\n",
    "    print(f\"loss:{loss}, reward:{reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a699012",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(C.state_dict(), 'controller-1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "world-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
