{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d03b9bd",
   "metadata": {},
   "source": [
    "# Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19892ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# --- 설정값 ---\n",
    "TOTAL_STEPS = 20000\n",
    "ENV_ID = 'LunarLander-v3'\n",
    "\n",
    "print(f\"'{ENV_ID}' 환경에서 총 {TOTAL_STEPS} 스텝의 데이터를 수집합니다.\")\n",
    "\n",
    "# --- 환경 초기화 ---\n",
    "env = gym.make(ENV_ID, render_mode='rgb_array')\n",
    "\n",
    "# --- 데이터 버퍼 ---\n",
    "obs_buffer = []\n",
    "frame_buffer = []\n",
    "action_buffer = []\n",
    "reward_buffer = []\n",
    "terminated_buffer = []\n",
    "truncated_buffer = []\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in tqdm(range(TOTAL_STEPS)):\n",
    "    action = env.action_space.sample()\n",
    "    next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # 버퍼에 저장\n",
    "    obs_buffer.append(observation)  # 상태 (벡터)\n",
    "    frame = env.render()            # 프레임 이미지\n",
    "    frame_buffer.append(frame)     # 이미지 추가\n",
    "\n",
    "    action_buffer.append(action)\n",
    "    reward_buffer.append(reward)\n",
    "    terminated_buffer.append(terminated)\n",
    "    truncated_buffer.append(truncated)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "    else:\n",
    "        observation = next_observation\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(f\"\\n데이터 수집 완료! 총 {len(obs_buffer)}개의 경험을 저장했습니다.\")\n",
    "\n",
    "# --- 저장 디렉토리 만들기 ---\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# --- 저장 ---\n",
    "np.savez_compressed(\n",
    "    f\"data/{ENV_ID}_with_frames_1.npz\",\n",
    "    observations=np.array(obs_buffer, dtype=np.float32),  # (100000, 8)\n",
    "    frames=np.array(frame_buffer, dtype=np.uint8),        # (100000, H, W, 3)\n",
    "    actions=np.array(action_buffer, dtype=np.int8),\n",
    "    rewards=np.array(reward_buffer, dtype=np.float32),\n",
    "    terminateds=np.array(terminated_buffer, dtype=np.bool_),\n",
    "    truncateds=np.array(truncated_buffer, dtype=np.bool_)\n",
    ")\n",
    "\n",
    "print(f\"✅ '{ENV_ID}_with_frames_1.npz' 파일로 프레임 포함 데이터 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('data/LunarLander-v3.npz')\n",
    "observations_raw = data['frames']\n",
    "actions = data['actions']\n",
    "rewards = data['rewards']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0985f6e3",
   "metadata": {},
   "source": [
    "# VAE train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af45596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from parts.VAE_CNN import VAE, vae_loss_function, CustomImageDataset\n",
    "from parts.MDN_RNN import MDN_RNN, mdn_rnn_loss\n",
    "from parts.controller import controller\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "observations = torch.tensor(observations_raw).permute(0, 3, 1, 2)\n",
    "vae = VAE(input_channel=3, latent_dim=1024).to('cuda:0')\n",
    "\n",
    "resize = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()]) \n",
    "\n",
    "observations = CustomImageDataset(data=observations, transform=resize)\n",
    "\n",
    "dataloader = DataLoader(dataset=observations, batch_size=2048)\n",
    "\n",
    "optimizer = optim.AdamW(vae.parameters(), lr=5e-5)\n",
    "\n",
    "def vae_train(vae, optimizer, dataloader, epochs=10):\n",
    "    vae.train()\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        total_loss = 0\n",
    "        for batch_idx, data in enumerate(dataloader):\n",
    "            data = data.to('cuda:0')\n",
    "            _, recon_image, mu, logvar = vae(data)\n",
    "\n",
    "            loss = vae_loss_function(recon_image, data, mu, logvar, beta=0.5)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch}, Everage loss: {total_loss/len(dataloader):.6f}')\n",
    "\n",
    "#vae_train(vae, optimizer, dataloader, epochs=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9365bc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), 'vae-latent1024-epoch100-beta1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c0e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.load_state_dict(torch.load('model_weights/vae-latent1024-epoch100_beta0.01.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6442fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil = transforms.ToPILImage()\n",
    "to_pil(observations[555])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc17b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "data = observations[555].unsqueeze(0).to(\"cuda:0\")\n",
    "hidden_state, recon_image, _, _ = vae(data)\n",
    "to_pil(recon_image[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d602a484",
   "metadata": {},
   "source": [
    "# MDN_RNN train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6cd3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('data/LunarLander-v3.npz')\n",
    "observations_raw = data['frames']\n",
    "actions = data['actions']\n",
    "rewards = data['rewards']\n",
    "\n",
    "episodes = [0]\n",
    "for idx, done in enumerate(data['terminateds']):\n",
    "    if done == True:\n",
    "        episodes.append(idx+1)\n",
    "episodes.append(len(data['terminateds']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1a449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "# observations = torch.tensor(observations_raw).permute(0, 3, 1, 2)\n",
    "observations = observations_raw # numpy array dimension order is [H, W, C] for ToPILImage()\n",
    "\n",
    "resize = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()]) \n",
    "\n",
    "import torch\n",
    "from parts.VAE_CNN import VAE\n",
    "vae = VAE(input_channel=3, latent_dim=1024).to('cuda:0')\n",
    "vae.load_state_dict(torch.load('model_weights/vae-latent1024-epoch100.pth'))\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4895f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from parts.MDN_RNN import MDN_RNN, mdn_rnn_loss, SequenceDataset\n",
    "import numpy as np\n",
    "\n",
    "action_size = int(actions.max()) + 1\n",
    "action_onehot = np.eye(action_size)[actions] # np.eye is generates identity matrix\n",
    "\n",
    "mdn_rnn = MDN_RNN(input_size=1024, hidden_size=512, latent_space_size=1024, action_size=action_size).to('cuda:0')\n",
    "\n",
    "seq_dataset = SequenceDataset(image_dataset=observations, transforms=resize, reward_dataset=rewards, action_dataset=action_onehot, episodes=episodes)\n",
    "dataloader = DataLoader(dataset=seq_dataset, batch_size=32, num_workers=12)\n",
    "\n",
    "optimizer = optim.AdamW(mdn_rnn.parameters(), lr=5e-5)\n",
    "\n",
    "def rnn_train(model=mdn_rnn, dataloader=dataloader, optimizer=optimizer, epochs=200):\n",
    "    model.train()\n",
    "    vae.eval()\n",
    "\n",
    "    R = 4\n",
    "    W_done = 1.0\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (image, action, reward, seq_length, mask, t_done) in enumerate(dataloader):\n",
    "            image, action, reward, mask, t_done = image.to('cuda:0'), action.to('cuda:0'), reward.to('cuda:0'), mask.to('cuda:0'), t_done.to('cuda:0')\n",
    "            done_weights = torch.ones_like(t_done, device=t_done.device, dtype=t_done.dtype) # Need to press the loss around done\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Have to reshape image vector because vae(cnn) input shape is (batch_size, channel_size, height, width)\n",
    "                batch, sequence, C, H, W = image.size()\n",
    "                reshape_images = image.view(-1, C, H, W)\n",
    "                z_vectors_flatten, _, _, _ = vae(reshape_images)\n",
    "                z_vectors = z_vectors_flatten.view(batch, sequence, -1)\n",
    "                \n",
    "            mu, sigma, phi, p_reward, p_done, _, (_, _) = model(z_vectors, action, length=seq_length)\n",
    "            \n",
    "            batch, sequence, num_dist, latent_size = mu.size()\n",
    "\n",
    "            # Done weights\n",
    "            for b, i in enumerate(seq_length.tolist()):\n",
    "                L = i - R\n",
    "                done_weights[b, L:i-1] = W_done\n",
    "                #print(done_weights[b])\n",
    "            done_weights = done_weights[:, :-1].reshape(-1, 1)\n",
    "\n",
    "            # Predict reshape\n",
    "            mu_pred = mu[: , :-1, :, :].reshape(-1, num_dist, latent_size)\n",
    "            sigma_pred = sigma[: , :-1, :, :].reshape(-1, num_dist, latent_size)\n",
    "            phi_pred = phi[: , :-1, :].reshape(-1, num_dist)\n",
    "            p_reward_pred = p_reward[:, :-1].reshape(-1, 1)\n",
    "            p_done_pred = p_done[:, :-1].reshape(-1, 1)\n",
    "\n",
    "            # Target reshape\n",
    "            target_z = z_vectors[: , 1:, :].reshape(-1, latent_size)\n",
    "            target_reward = reward[:, 1:].reshape(-1, 1)\n",
    "            target_done = t_done[:, 1:].reshape(-1, 1)\n",
    "\n",
    "            # Mask\n",
    "            reshape_mask = mask[:, :-1].reshape(-1, 1)\n",
    "\n",
    "            loss, (likelihood, reward_mse, done_bce) = mdn_rnn_loss(mu=mu_pred, sigma=sigma_pred, phi=phi_pred, \n",
    "                                                                    target=target_z, \n",
    "                                                                    p_reward=p_reward_pred, t_reward=target_reward, \n",
    "                                                                    p_done=p_done_pred, t_done=target_done, \n",
    "                                                                    done_weights=done_weights, \n",
    "                                                                    mask=reshape_mask, \n",
    "                                                                    likelihood_weight=2e-6)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f\"loss {total_loss / (batch_idx + 1)}\")\n",
    "\n",
    "        print(f\"epoch: {epoch+1}, Everage loss: {total_loss/len(dataloader):.6f}\")\n",
    "\n",
    "rnn_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d0288",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mdn_rnn.state_dict(), 'model_weights/mdnrnn_latent1024-epoch200.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f70decd",
   "metadata": {},
   "source": [
    "# RNN test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc01a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('data/LunarLander-v3.npz')\n",
    "observations_raw = data['frames']\n",
    "actions = data['actions']\n",
    "rewards = data['rewards']\n",
    "\n",
    "episodes = [0]\n",
    "for idx, done in enumerate(data['terminateds']):\n",
    "    if done == True:\n",
    "        episodes.append(idx+1)\n",
    "episodes.append(len(data['terminateds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc3e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from parts.VAE_CNN import VAE, vae_loss_function, CustomImageDataset\n",
    "from parts.MDN_RNN import MDN_RNN, mdn_rnn_loss, sampling, SequenceDataset\n",
    "from parts.controller import controller\n",
    "\n",
    "action_size = int(actions.max()) + 1\n",
    "action_onehot = np.eye(action_size)[actions] # np.eye is generates identity matrix\n",
    "\n",
    "vae = VAE(input_channel=3, latent_dim=1024).to('cuda:0')\n",
    "mdn_rnn = MDN_RNN(input_size=1024, hidden_size=512, latent_space_size=1024, action_size=action_size).to('cuda:0')\n",
    "\n",
    "vae.load_state_dict(torch.load('model_weights/vae-latent1024-epoch100.pth'))\n",
    "mdn_rnn.load_state_dict(torch.load('model_weights/mdnrnn_latent1024-epoch200.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db214c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "observations = observations_raw\n",
    "\n",
    "resize = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()]) \n",
    "\n",
    "\n",
    "seq_dataset = SequenceDataset(image_dataset=observations, transforms=resize, reward_dataset=rewards, action_dataset=action_onehot, episodes=episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec62d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "mdn_rnn.eval()\n",
    "z_vectors, _, _, _  = vae(seq_dataset[100][0].to('cuda:0'))\n",
    "z_vectors = z_vectors.unsqueeze(0)\n",
    "action_first = seq_dataset[100][1].to('cuda:0')\n",
    "action_first = action_first.unsqueeze(0)\n",
    "\n",
    "mu, sigma, phi, reward, done, h, cell = mdn_rnn(z_vectors, action_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil = transforms.ToPILImage()\n",
    "to_pil(seq_dataset[2][0][seq_dataset[2][3]-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a54bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "done, last_seq = done_pred(22)\n",
    "print(F.sigmoid(done))\n",
    "print(last_seq)\n",
    "print(F.sigmoid(done[:, last_seq-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21735f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def done_pred(index):\n",
    "    last_seq = seq_dataset[index][3].to('cuda:0')\n",
    "    z_vectors, _, _, _  = vae(seq_dataset[index][0].to('cuda:0'))\n",
    "    action = seq_dataset[index][1].to('cuda:0')\n",
    "    z_vectors = z_vectors.unsqueeze(0)\n",
    "    action = action.unsqueeze(0)\n",
    "    mu, sigma, phi, reward, done, h, cell = mdn_rnn(z_vectors, action)\n",
    "    return done, last_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd559760",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = phi[:, 0, :]\n",
    "m = mu[:, 0, :, :]\n",
    "s = sigma[:, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de9795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import distributions\n",
    "from torchvision import transforms\n",
    "\n",
    "mixture_distribution = distributions.Categorical(probs=p)\n",
    "component_distribution = distributions.Normal(loc=m, scale=s)\n",
    "component_dist = distributions.Independent(\n",
    "    component_distribution,\n",
    "    reinterpreted_batch_ndims=1\n",
    ")\n",
    "mixture_gaussian = distributions.MixtureSameFamily(mixture_distribution, component_dist)\n",
    "vector = mixture_gaussian.sample()\n",
    "\n",
    "to_pil = transforms.ToPILImage()\n",
    "a = vae.decode(vector)\n",
    "img = to_pil(a[0])\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c0e77",
   "metadata": {},
   "source": [
    "# Controller train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676540e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from parts.MDN_RNN import MDN_RNN, mdn_rnn_loss, sampling, SequenceDataset\n",
    "from parts.controller import controller, choice_control\n",
    "from parts.VAE_CNN import VAE\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "data = np.load('data/LunarLander-v3.npz')\n",
    "\n",
    "actions = data['actions']\n",
    "action_size = int(actions.max()) + 1\n",
    "action_onehot = np.eye(action_size)[actions]\n",
    "\n",
    "rewards = data['rewards']\n",
    "\n",
    "images = data['frames']\n",
    "\n",
    "episodes = [0]\n",
    "for idx, done in enumerate(data['terminateds']):\n",
    "    if done == True:\n",
    "        episodes.append(idx+1)\n",
    "episodes.append(len(data['terminateds']))\n",
    "\n",
    "resize = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()]) \n",
    "\n",
    "seq_dataset = SequenceDataset(image_dataset=images, transforms=resize, reward_dataset=rewards, action_dataset=action_onehot, episodes=episodes)\n",
    "\n",
    "mdn_rnn = MDN_RNN(input_size=1024, hidden_size=512, latent_space_size=1024, action_size=action_size).to('cuda:0')\n",
    "mdn_rnn.load_state_dict(torch.load('model_weights/mdnrnn_latent1024-epoch200.pth'))\n",
    "\n",
    "vae = VAE(input_channel=3, latent_dim=1024).to('cuda:0')\n",
    "vae.load_state_dict(torch.load('model_weights/vae-latent1024-epoch100-beta0.01.pth'))\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257e0028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import random\n",
    "\n",
    "def sellect_random_scene(images) :\n",
    "    _len = len(images)\n",
    "    scene_number = random.randint(0, _len-1)\n",
    "    image = images[scene_number]\n",
    "    \n",
    "    image_tensor = torch.tensor(image).permute(2, 0, 1)\n",
    "\n",
    "    resized_image = torch.tensor(resize(image_tensor)).unsqueeze(0).to('cuda:0')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z_vector, _ ,_ ,_ = vae(resized_image)\n",
    "\n",
    "    num_layers, hidden_size = mdn_rnn.lstm.num_layers, mdn_rnn.lstm.hidden_size\n",
    "\n",
    "    h_vector = torch.zeros(1, hidden_size).to('cuda:0')\n",
    "\n",
    "    h_n, c_n = torch.zeros(num_layers, 1, hidden_size).to('cuda:0'), torch.zeros(num_layers, 1, hidden_size).to('cuda:0')\n",
    "\n",
    "    return z_vector, h_vector, (h_n, c_n)\n",
    "\n",
    "def initial_scene(seq_dataset, rnn):\n",
    "    scene_number = random.randint(0, len(seq_dataset)-1)\n",
    "    image0 = seq_dataset[scene_number][0][0]\n",
    "    image1 = seq_dataset[scene_number][0][1]\n",
    "\n",
    "    tensor_image0 = image0.unsqueeze(0).to('cuda:0')\n",
    "    tensor_image1 = image1.unsqueeze(0).to('cuda:0')\n",
    "\n",
    "    action = seq_dataset[scene_number][1][0]\n",
    "    action_len = len(action)\n",
    "    random_action = random.randint(0, action_len - 1)\n",
    "    action_onehot = torch.zeros(1, action_len).to('cuda:0')\n",
    "    action_onehot[0][random_action] = 1.0\n",
    "    action_onehot = action_onehot.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z_vector0, _ ,_ ,_ = vae(tensor_image0)\n",
    "        z_vector1, _ ,_ ,_ = vae(tensor_image1)\n",
    "    z_vector0 = z_vector0.unsqueeze(0)\n",
    "\n",
    "    _, _, _, _, _, h_vector, (h_n, c_n) = rnn(z_vector0, action_onehot)\n",
    "\n",
    "    return z_vector1, h_vector, (h_n, c_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8658277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parts.MDN_RNN import MDN_RNN, sampling\n",
    "from parts.controller import controller, choice_control\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "C = controller(action_size=action_size, h_vector_size=512, z_vector_size=1024, hidden_size=512).to('cuda:0')\n",
    "\n",
    "optimizer = torch.optim.AdamW(C.parameters(), lr=1e-4)\n",
    "\n",
    "gamma = 0.99\n",
    "beta = 0.1\n",
    "\n",
    "def controller_train(steps, controller, optimizer, initial_z, initial_h, cell):\n",
    "    controller.train()\n",
    "    mdn_rnn.eval()\n",
    "\n",
    "    rewards = []\n",
    "    action_probs = []\n",
    "    entropies = []\n",
    "    values = [] # baseline values\n",
    "\n",
    "    z = initial_z.to('cuda:0').detach()\n",
    "    h = initial_h.to('cuda:0').detach()\n",
    "\n",
    "    z_rnn = z.unsqueeze(0)\n",
    "\n",
    "    for i in range(steps):\n",
    "        action_prob, value = controller(z, h)\n",
    "        action = choice_control(action_prob)\n",
    "\n",
    "        action_onehot_tensor = F.one_hot(action, num_classes=action_size).to(torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mu, sigma, phi, reward, done, h, cell = mdn_rnn(z_rnn, action_onehot_tensor, cell)\n",
    "\n",
    "            z_rnn = sampling(mu, sigma, phi) \n",
    "            z = z_rnn.squeeze(0).detach()\n",
    "            h = h.detach()\n",
    "\n",
    "        log_action_prob = torch.log(action_prob[0, action])\n",
    "\n",
    "        entropy = action_prob[0] * torch.log(action_prob[0])\n",
    "\n",
    "        # print(z)\n",
    "        # to_pil = transforms.ToPILImage()\n",
    "        # a = vae.decode(z)\n",
    "        # img = to_pil(a[0])\n",
    "        # plt.figure(figsize=(5, 5))\n",
    "        # plt.imshow(img)\n",
    "        # plt.axis('off')\n",
    "        # plt.show()\n",
    "        \n",
    "        entropies.append(-entropy.sum())\n",
    "        action_probs.append(log_action_prob)\n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "\n",
    "        if F.sigmoid(done) >= 0.96:\n",
    "            #print(f\"Episode finished with reward: {reward.item()}\")\n",
    "            #print(i)\n",
    "            break\n",
    "\n",
    "    returns = []\n",
    "    future_return = 0.0\n",
    "    for r in reversed(rewards):\n",
    "        future_return = r + gamma * future_return\n",
    "        returns.append(future_return)\n",
    "    returns.reverse()\n",
    "    returns = torch.stack(returns)\n",
    "    values = torch.stack(values)\n",
    "\n",
    "    adv = returns - values.detach()\n",
    "    \n",
    "    adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "    action_probs = torch.stack(action_probs)\n",
    "    entropies = torch.stack(entropies)\n",
    "\n",
    "    policy_loss = - (action_probs * adv.detach()).mean() # 정책 손실\n",
    "    entropy_loss = - (beta * entropies.mean()) # 엔트로피 손실 (엔트로피를 최대화하기 위해 음수로 만듦)\n",
    "    value_loss = 0.5 * (returns - values).pow(2).mean()\n",
    "\n",
    "    loss = policy_loss + entropy_loss + value_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(controller.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), torch.stack(rewards).sum().item(), policy_loss.item(), entropy_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5087c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for _ in tqdm(range(1000)):\n",
    "    initial_z, initial_h, (h_n, c_n) = initial_scene(seq_dataset, mdn_rnn)\n",
    "    loss, reward, policy, entropy = controller_train(140, C, optimizer, initial_z, initial_h, (h_n, c_n))\n",
    "    print(f\"loss:{loss}, reward:{reward}, policy:{policy*1000}, entropy:{entropy*1000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a699012",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(C.state_dict(), 'controller-1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "world-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
